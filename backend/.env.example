# Local development with Docker/K8s PostgreSQL
DATABASE_URL="postgresql://postgres:postgres@localhost:5432/puppystore?schema=public"

# Azure PostgreSQL (production)
# DATABASE_URL="postgresql://USER:PASSWORD@HOSTNAME.postgres.database.azure.com:5432/puppystore?schema=public&sslmode=require"

# =============================================================================
# LLM Configuration
# =============================================================================

# OpenAI (production)
OPENAI_API_KEY=sk-xxx
OPENAI_MODEL=gpt-4o-mini

# Local LLM (development) - uncomment to use local model
# LLM_PROVIDER=local
# LOCAL_LLM_BASE_URL=http://127.0.0.1:11434/v1
# LOCAL_LLM_MODEL=qwen2.5-3b-instruct

# =============================================================================
# Embeddings Configuration
# =============================================================================

# Embedding provider: 'openai' or 'local'
EMBEDDING_PROVIDER=local

# OpenAI embeddings (production)
OPENAI_EMBEDDING_MODEL=text-embedding-3-small
EMBEDDING_DIMENSIONS=768  # Can use 768 for cost savings (default 1536)

# Local embeddings (development) - llama.cpp with nomic-embed-text
LOCAL_EMBEDDING_BASE_URL=http://127.0.0.1:8081/v1
LOCAL_EMBEDDING_MODEL=nomic-embed-text-v1.5

# =============================================================================
# Semantic Cache Configuration
# =============================================================================

# Similarity threshold for cache hits (0.0-1.0, higher = stricter matching)
SEMANTIC_CACHE_THRESHOLD=0.85

# Maximum cache entries before LRU eviction kicks in
SEMANTIC_CACHE_MAX_ENTRIES=10000

# =============================================================================
# Observability
# =============================================================================

# Sentry - Error tracking (https://sentry.io)
SENTRY_DSN=https://xxx@o123.ingest.sentry.io/456

# LangSmith - LLM tracing (https://smith.langchain.com)
LANGCHAIN_API_KEY=lsv2_pt_xxxx
LANGCHAIN_PROJECT=puppy-store

# Loki - Log aggregation (local docker)
LOKI_HOST=http://localhost:3100

# Logging
LOG_LEVEL=debug

# =============================================================================
# Redis
# =============================================================================

REDIS_URL=redis://localhost:6379

# TTL configuration (in seconds)
REDIS_TTL_CHAT_SESSION=3600         # 1 hour - chat sessions
REDIS_TTL_RECOMMEND_SESSION=3600    # 1 hour - recommendation sessions
REDIS_TTL_DESCRIPTION_CACHE=86400   # 24 hours - AI-generated descriptions

# =============================================================================
# Application
# =============================================================================

NODE_ENV=development
APP_VERSION=1.0.0
